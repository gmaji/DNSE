package sna.dns.experiment;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import sna.physica.graph.IndexedGraph;
import sna.physica.ranking.DegreePlusRanker;
import sna.physica.ranking.DegreeRanker;
import sna.physica.ranking.DirectionalNodeStrengthEntropy;
import sna.physica.ranking.EigenVectorRanker;
import sna.physica.ranking.EpidemicRanker;
import sna.physica.ranking.ImprovedRanker;
import sna.physica.ranking.KShellPlusRanker;
import sna.physica.ranking.KShellRanker;
import sna.physica.ranking.LSS_Ullah_Ranker;
import sna.physica.ranking.MDDRanker;
import sna.physica.ranking.PageRankRanker;
import sna.physica.ranking.PotentialEdgeWtKShellRanker;
import sna.physica.ranking.WtKsDegreeNeighborhood_Giridhar;

public class AverageInfluenceByFraction {
	/***
	 * It gives the Average (or total) Collective spreading Influence per node in terms of total number of infected nodes after the Infection dies out 
	 * in SIR model. Results are with overlap of influence.
	 * It starts with a nodeSet that contains top f fraction of nodes generated by different ranking heuristics and then calculate
	 * total number of Infected nodes by adding the Individual node's influences (i.e. the number of Infected nodes when that node 
	 * has started the Infection). Seed nodes are considered as fraction/percentage of total nodes in the network. we have considered
	 * p values as 0.01 to 0.1 i.e. 1% to 10% seed nodes. For each such starting node SIR model is run for 1000 times and average 
	 * number of Infected nodes are taken as Influence of that node. 
	 * Important point to note: Here we first identify the seed nodeset using different ranking heuristics, and then for each node 
	 * simulate SIR model 1000 times to get its Individual Influence in terms of average number of infected nodes. Finally, all 
	 * such individual influences of seed nodeset are added up and averaged to find the Average Influence M of some ranking method.
	 * @param args
	 * @throws Exception
	 * 
	 * Please see.
	 *  Pei, S., Muchnik, L., Andrade, J.S., Zheng, Z.M. & Makse, H.A. Searching for superspreaders of information in real-world 
	 *  social media. Sci. Rep. 4, 5547; DOI:10.1038/srep05547 (2014).
	 */
	public static void main(String[] args) throws Exception {
		
		long startTime = System.currentTimeMillis();

String[][] networks = {
//   {"Network", 	"dataset_file",    "beta_th",   "mu",         "beta_exp" },
//	{"Brightkite",	"brightkite.txt",	"0.016",    "0.40",         "0.31"}, // V: 58228 E:214078
//	{"Douban",	    "douban.txt",	    "0.027",    "0.40",         "0.31"}, // V:154908  E:183831 (undirected) E:367662(directed)
	{"NetScience",	"netscience.txt",	"0.144",    "0.4",	   	    "0.15"}, //correct nodes:1589, E:2742; beta no- change
	{"C.elegans",	"celegans.txt",		"0.038",	"0.40",   	    "0.06"}, // V:306 E:2148 after removing duplicate edges and undirected V:297 E: 2148
//	{"E-mail",		"email.txt",		"0.007",	"0.40",		    "0.15"}, //Nodes: 36692 Edges: 367662
	{"Pol-Blogs",	"pollblog.txt",		"0.012",	 "0.40",		"0.15"},//V: 1490 E:16718
//	{"Advogato",	"advogato.txt",		"0.012",     "0.9",         "0.05"}, //V:6541, E:51127, beta no change
//	{"CA-AstroPh",	"ca_astroph.txt",	"0.015",	 "0.01",		"0.02"}, // Undirected, Un-weighted; V: 18772, E:198050 ; correct beta no change
//	{"CA-CondMat2005",	"condmat05.txt","0.037",	 "0.4",         "0.07"},// V: 40421 E:175692
//	{"CA-CondMat",	"condmat.txt",	    "0.077",	 "0.4",         "0.07"},// V: 16726 E:47594 undirected
//	{"Amazon MDS",	"amazon_mds.txt", 	"0.087",	 "0.4",		    "0.15"}, //  Undirected, Un-weighted, No-loop ; V: 925872 E:334863
	{"Zachary",		"zachary.txt", 		"0.129",	 "0.4",         "0.05"},//  Undirected, Un-weighted, No-loop ; V:34 E:78
	{"Powergrid",	"powergrid.txt", 	"0.258",	 "0.4",		    "0.30"}, // Undirected, Un-weighted, V:4941  E:6594
	{"Hamsterter", 	"hamster.txt",		"0.022",	 "0.1",		    "0.05"}, // V:1858 E: 12534 beta no change
	{"US-Airport",  "usairport.txt", 	"0.009",	 "0.4",         "0.16"}, // not undirected; V: 1574; E: 17215
	{"US-Airline-97","usairline97.txt",	"0.023",	 "0.4",         "0.05"}, //correct data; V:332  E:2126; beta-no change
	{"Jazz",	    "jazz.txt", 	    "0.026",	 "0.4",		    "0.05"}, // Undirected, Un-weighted, correct data; V:198 E:2742
	{"Dolphins",	"dolphins.txt",		"0.147",	 "0.4",		    "0.05"}, // Undirected, Un-weighted; V:62  E:159
//	{"DBLP-CITE",	"dblp_cite.txt",	"0.023",	 "0.1",		    "0.05"}, //correct data; V:12591 E:49743
//	{"DBLP-CoAuthor","dblp_ca.txt",    	"0.046",	 "0.1",		    "0.05"}, //  Undirected, Un-weighted, V:317080 E:1049866; beta_th
	{"EURO-Road",	"euroroad.txt",	    "0.333",	 "0.4",         "0.05"}, // V: 1174; E:1417
	{"High-School",	"highschool.txt",   "0.107",	 "0.4",         "0.05"},    // V: 70; E:274   
	{"Foodweb",		"foodweb.txt",	    "0.023",	 "0.4",         "0.05"}, // V: 183; E: 2434
	{"Macaques",	"macaques.txt",	    "0.026",	 "0.4",         "0.05"}, // V: 65; E:1169      
	{"PGP",	    	"pgp.txt",	        "0.053",	 "0.4",		    "0.11"},// V: 10680  E: 24316
	{"Football",	"football.txt",	    "0.093",	 "0.4",		    "0.35"},// Undirected un weighted V: 115;  E:613
	{"Zebra",	    "zebra.txt",	    "0.091",	 "0.4",		    "0.35"},// V:27;  E:111
	{"Odlis",	    "odlis.txt",	    "0.014",	 "0.9",		    "0.35"},// V:2909;  E:18241; beta no change
//	{"Test",	    "test.txt",	        "0.31",		 "0.4",		    "0.35"},// V:13;  E:18
	{"Toy_DNS",		"toy_dns.txt",	    "0.276",	 "0.4",		    "0.30"},  					    
					};

		for (int i=0; i < networks.length; i++) {

			System.err.println(networks[i][0]);
			String filename = "dataset/undirected/" + networks[i][1];
			IndexedGraph graph = IndexedGraph.readGraph(filename);
			int N = graph.getVertexCount();
			//double beta_exp = Double.parseDouble(networks[i][4]);
			double beta_th = Double.parseDouble(networks[i][2]);
			double beta_exp = 1.5 * beta_th ;
			PrintStream out = new PrintStream(new File("results/total_ci."+networks[i][1]));
			
			int iteration = 1000;
			if (graph.getEdgeCount() > 10000) {
				iteration = 100;
			}
			EpidemicRanker epidemicRanker = new EpidemicRanker(graph, beta_exp, iteration);
			epidemicRanker.evaluate();	
			
			DegreeRanker degreeRanker = new DegreeRanker(graph);
			degreeRanker.evaluate();
			
			DegreePlusRanker degreePlusRanker = new DegreePlusRanker(graph);
			degreePlusRanker.evaluate();
	
			KShellRanker kshellRanker = new KShellRanker(graph);
			kshellRanker.evaluate();
			
			KShellPlusRanker kshellplusRanker = new KShellPlusRanker(graph);
			kshellplusRanker.evaluate();
			
			double alpha=0.15; // setting alpha=0.15 is same as setting damping factor = 0.85 = (1-alpha)
			PageRankRanker pageRanker = new PageRankRanker(graph, alpha);
			pageRanker.evaluate();
			
		   // PageRank with alpha=0.0 yield same score as EigenVectorRanker
			EigenVectorRanker  evRanker = new EigenVectorRanker(graph);
			evRanker.evaluate();
			
			LSS_Ullah_Ranker lss = new LSS_Ullah_Ranker(graph);
			lss.evaluate();
			/*
			PotentialEdgeWtKShellRanker kshellW = new PotentialEdgeWtKShellRanker(graph);
			kshellW.evaluate();
			
			double lambda1 = 0;
		    WtKsDegreeNeighborhood_Giridhar ksW_GM = new WtKsDegreeNeighborhood_Giridhar(graph,lambda1, 1000);
			ksW_GM.evaluate();
			
			
			double lambda = 0.7;
			MDDRanker mddRanker = new MDDRanker(graph, lambda);
			mddRanker.evaluate();

			ImprovedRanker improvedRanker = new ImprovedRanker(graph);
			improvedRanker.evaluate();
			
			DirectionalNodeStrengthEntropy dns2Ranker = new DirectionalNodeStrengthEntropy(graph, lambda, 10000);
			dns2Ranker.evaluate();
			*/
			
			//out.printf("beta \t  p \t K   \t KS  \t KS+ \t KSW \t KSWGM \t MDD \t Theta \t DNSv2 \t SeedSize\n");	
			out.printf("beta \t  p \t K   \t KS  \t KS+ \t PR \t EigenV \t LSS \t SeedSize\n");
			for(int k=2;k<=20; k+=2){
			//for(int k=1;k<=5; k+=1){
	/* p is the percentage of total nodes used as seed node. p=0.01 means 1% of the top ranked nodes are used as seed nodes
	 * Here we shall consider 1% to 5% of the total nodes(|V|) as seed nodes for large networks.
	 */		   double p = 0.0; 
				if(k==0){
				 p = 0.01;
				}else{
					p = k*0.01;	
				}
		        out.printf("%1.3f \t %1.2f \t", beta_exp,p);
		        
		        List<Set<Integer>> allNodeSet = new ArrayList<Set<Integer>>();
		       int q=0;
				allNodeSet.add(q++,degreeRanker.getTopSeedNodeSetByFraction(p));
				allNodeSet.add(q++,kshellRanker.getTopSeedNodeSetByFraction(p));
				allNodeSet.add(q++,kshellplusRanker.getTopSeedNodeSetByFraction(p));
				allNodeSet.add(q++,pageRanker.getTopSeedNodeSetByFraction(p));
				allNodeSet.add(q++,evRanker.getTopSeedNodeSetByFraction(p));
				allNodeSet.add(q++,lss.getTopSeedNodeSetByFraction(p));
				//allNodeSet.add(q++,kshellW.getTopSeedNodeSetByFraction(p));
				//allNodeSet.add(q++,ksW_GM.getTopSeedNodeSetByFraction(p));
				//allNodeSet.add(q++,mddRanker.getTopSeedNodeSetByFraction(p));
				//allNodeSet.add(q++,improvedRanker.getTopSeedNodeSetByFraction(p));
				//allNodeSet.add(q++,getSeedNodeSetByFractionOfCI(networks[i][1],graph.getVertexCount(),p));
				//allNodeSet.add(q,dns2Ranker.getTopSeedNodeSetByFraction(p));
				
				int seedSize = 0;
				for(int x=0; x<allNodeSet.size(); x++){
					Set<Integer> nodeSet = allNodeSet.get(x);
				   // System.out.println("\n x= "+x+"  nodeSet.size()::"+nodeSet.size());
				    epidemicRanker.evaluate(nodeSet);
				    double ci=0.0;
				     seedSize = nodeSet.size();
				     for(Integer vt: nodeSet){
					  // System.out.println("Node Name: "+graph.getVertex(vt)+"   Node#: "+graph.getIndex(graph.getVertex(vt)));
					  // System.out.println("Node Value by Index::: "+epidemicRanker.getNodeValue(vt));
					  // System.out.println("Node Value by Name::: "+epidemicRanker.getNodeValueDouble(graph.getVertex(vt)));
					   ci+=epidemicRanker.getNodeValueDouble(graph.getVertex(vt));	
				       }
				     //out.printf("%1.2f \t",ci/seedSize); // for avg_ci_per_seed_node
				     out.printf("%1.2f \t",ci);// for cumulative spreading influence of the seeds
				    // out.printf("%1.2f \t",ci/(seedSize*N)); // to get the total spread as a fraction of total nodes in the network 
				  }
				  out.printf("%d", seedSize);
				  out.println();
				  System.out.println("\n nodeSet.size()::"+seedSize);
				
				//System.out.println("CI::without Intersection "+p*100+"% #SeedNodes:= "+nodeSet.size()+" Total Infected Population:= "+ci1);
				//System.out.println("CI::with "+p*100+"% #SeedNodes:= "+nodeSet.size()+" Total Infected Population:= "+ci2);
			
				
			}
			out.close();
		}
		
		long endTime = System.currentTimeMillis();
		System.out.printf("Elapsed Time : %d secs", (endTime-startTime)/1000);

	}
		
	private static Set<Integer> getSeedNodeSetByFractionOfCI(String fName, int verteCnt, double p) throws IOException{
		Set<Integer> nodeSet = new HashSet<Integer>();
		String fileName = "CI_CPP/dataset/" +fName+"_out";
		//int seedNodeCount = (int)(verteCnt*p); 
		int seedNodeCount = (int)(Math.round(verteCnt*p)); 
		System.out.println("Vertex Count: "+verteCnt+"  seedNodeCount:="+seedNodeCount);
		BufferedReader reader = new BufferedReader(new FileReader(fileName));
		int count = 0;
		 String[] nodes= null;
		while (reader.ready()) {
			String line = reader.readLine();
			if (line.trim().length() == 0) continue;
			//if (line.trim().startsWith("#")) continue;
			/*StringTokenizer st = new StringTokenizer(line.trim(), ",");
			String vi = st.nextToken().trim();
			String vj = st.nextToken().trim();*/
		    nodes = line.split(",");
		}
		reader.close();
		for(int z=1; z<=seedNodeCount; z++){
			nodeSet.add(Integer.parseInt(nodes[z]));	
		}
		return nodeSet;
	}
}
